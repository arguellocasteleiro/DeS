# -*- coding: utf-8 -*-
"""DeS_MT_BlueTransformation.ipynb

Automatically generated by Colab.

# ___________________________ Requirements
### install libraries & load libraries
import torch

!pip install transformers
from transformers import pipeline, AutoTokenizer, AutoProcessor, SeamlessM4TModel
from IPython.display import Audio

!pip install sacrebleu
!pip install evaluate
import evaluate

# ___________________________
### Exemplifying MT English to French with an updated Google’s T5 (Text-To-Text Transfer Transformer). After execution, the MT output "Alimentation de précision" is correct. Google’s T5-Base is a LLM with 223 million parameters and a transformers encoder-decoder architecture.

###--- --- --- Requirements
# !pip install transformers
# from transformers import pipeline

###--- --- --- 3 lines of code
#1 instantiate pipeline
pipe = pipeline(task="translation_en_to_fr", model="google-t5/t5-base")

#2 input data
text_data = "Precision feeding"

#3 show pipeline output
print(pipe(text_data)[0]['translation_text'])

# ___________________________
### Exemplifying MT English to Spanish with an OPUS-MT model. After execution, the MT output "Alimentación de precisión" is correct. Helsinki-NLP/opus-mt-tc-big-en-es has 233 million parameters and is a base transformer model for translating from English to Spanish.

###--- --- --- Requirements
# !pip install transformers
# from transformers import pipeline

###--- --- --- 3 lines of code
#1 instantiate pipeline
pipe=pipeline(task="translation",model="Helsinki-NLP/opus-mt-tc-big-en-es")

#2 input data
text_data = "Precision feeding"

#3 show pipeline output
print(pipe(text_data)[0]['translation_text'])

# ___________________________
### Exemplifying MT English to Chinese with falcon-7b-instruct and zero-shot. After execution, the MT output is "准确喂". The falcon-7b-instruct is a decoder-only model with 7 billion parameters.

###--- --- --- Requirements
# import torch
# !pip install transformers accelerate
# from transformers import pipeline, AutoTokenizer

###--- --- --- 3 lines of code
#1 instantiate pipeline
pipe = pipeline(task="text-generation", model="tiiuae/falcon-7b-instruct", tokenizer=AutoTokenizer.from_pretrained("tiiuae/falcon-7b-instruct"), torch_dtype=torch.bfloat16, device_map="auto")

#2 input data
text_prompt = "Translate the English text to Chinese. Text: Precision feeding. Translation:"

#3 show pipeline output
print(pipe(text_prompt)[0]['generated_text'])

# ___________________________
### Exemplifying zero-shot prompting for MT from English to Spanish, French, and Chinese

###--- --- --- No-Code AI (prompting)

# MT from EN to ES
text_prompt = "Translate the English text to Spanish. Text: algae. Translation:"

# MT from EN to FR
text_prompt = "Translate the English text to French. Text: algae. Translation:"

# MT from EN to CN
text_prompt = "Translate the English text to Chinese. Text: algae. Translation:"

# ___________________________
### Exemplifying MT English to French with Google’s Flan-T5 model and zero-shot prompting. Google’s Flan-T5-Base is an encoder-decoder model with 248 million parameters.

###--- --- --- Requirements
# !pip install transformers
# from transformers import pipeline

###--- --- --- 3 lines of code
#1 instantiate pipeline
pipe = pipeline(task="text2text-generation", model="google/flan-t5-base")

#2 input data
text_prompt = "Translate from English to French: Precision feeding"

#3 show pipeline output
print(pipe(text_prompt, max_length=20)[0]['generated_text'])

# ___________________________
### Exemplifying MT English to Spanish with Facebook's SeamlessM4T. After execution, the MT output "Alimentación de precisión" is correct. Facebook's SeamlessM4T-Large has 2.3 billion parameters and the SeamlessM4T-v2 versatile architecture, including encoders and decoders, for sequential generation of text and speech.

###--- --- --- Requirements
# !pip install transformers
# from transformers import AutoProcessor, SeamlessM4TModel

#0 Preliminaries: specify model & processor
model = SeamlessM4TModel.from_pretrained("facebook/hf-seamless-m4t-large")
processor = AutoProcessor.from_pretrained("facebook/hf-seamless-m4t-large")

###--- --- --- 3 lines of code: text-to-text translation
#1 encode input
text_inputs = processor(text="Precision feeding", src_lang="eng", return_tensors="pt")

#2 model output
output_tokens = model.generate(**text_inputs, tgt_lang="spa", generate_speech=False)

#3 show decode output
print(processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True))

# ___________________________
### Exemplifying MT English to Spanish with Facebook's SeamlessM4T.

###--- --- --- Requirements
# !pip install transformers
# from transformers import AutoProcessor, SeamlessM4TModel
# from IPython.display import Audio

#0 Preliminaries: specify model & processor
model = SeamlessM4TModel.from_pretrained("facebook/hf-seamless-m4t-large")
processor = AutoProcessor.from_pretrained("facebook/hf-seamless-m4t-large")

###--- --- --- 3 lines of code: text-to-speech translation
#1 encode input
text_inputs = processor(text="Precision feeding", src_lang="eng", return_tensors="pt")

#2 model output
output_audio = model.generate(**text_inputs, tgt_lang="spa")[0].cpu().numpy().squeeze()

#3 load and play speech (audio created)
Audio(output_audio, rate=model.config.sampling_rate)

# ___________________________
### Exemplifying CHRF score with the MT output (the prediction) from Google’s Flan-T5-Base and the reference (the ground truth) from the multilingual FAO glossary created.

###--- --- --- Requirements
# !pip install sacrebleu
# !pip install evaluate
# import evaluate

###--- --- --- 3 lines of code
#1 load metric
chrf = evaluate.load("chrf")

#2 input data
predictionsMT =["alimentation précision"]
referencesFAO =["alimentation de précision"]

#3 show metric score
print(chrf.compute(predictions=predictionsMT, references=referencesFAO, word_order=0))